## í•œêµ­ì–´ì§€ì› ëª¨ë¸ í…ŒìŠ¤íŠ¸

### 1. í…ŒìŠ¤íŠ¸ í™˜ê²½

- GPU Node :

- Jupyter Notebook Image : CUDA v12.1, Python v3.11

- LLM Model: ollama

- s3 Storeage : minio

- dataset : pandas dataset í™œìš©

### 2. Jupyter Notebook Python Code

- íŒ¨í‚¤ì§€ ë‹¤ìš´ë¡œë“œ

  ```yaml
  # í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
  !pip install pandas langchain langchain-community langchain-chroma sentence-transformers torch
  !pip install --upgrade pip
  !pip install langchain-huggingface
  !pip install pysqlite3-binary
  !pip install --upgrade chromadb langchain-community
  !pip install gradio
  ```
  
- í™˜ê²½êµ¬ì„±

  ```
  
  import os
  import sys
  import torch
  import boto3
  import pandas as pd
  import gradio as gr
  from langchain_community.document_loaders import DataFrameLoader
  from langchain_text_splitters import RecursiveCharacterTextSplitter
  from langchain_community.embeddings import HuggingFaceBgeEmbeddings
  from langchain_community.vectorstores import Chroma
  from langchain.prompts import PromptTemplate
  from langchain_community.chat_models import ChatOllama
  from langchain.schema.runnable import RunnablePassthrough
  from langchain.schema.output_parser import StrOutputParser
  
  # pysqlite3 ì„¤ì • (Chromaì—ì„œ sqlite3 ì´ìŠˆ ë°©ì§€)
  __import__('pysqlite3')
  sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
  
  ```

  

- ë°ì´í„°ì…‹ ë¡œë“œ

  ```yaml
  # Titanic ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
  csv_file_path = './data/titanic.csv'
  df = pd.read_csv(csv_file_path)
  
  # ë°ì´í„° ê°€ê³µ
  df['Survived_str'] = df['Survived'].apply(lambda x: 'ìƒì¡´' if x == 1 else 'ì‚¬ë§')
  df['combined_info'] = (
      df['Name'] + "ì€(ëŠ”) " +
      df['Sex'] + "ì„± ìŠ¹ê°ìœ¼ë¡œ, ë‚˜ì´ëŠ” " + df['Age'].astype(str) + "ì„¸ì…ë‹ˆë‹¤. " +
      "íƒ‘ìŠ¹ ë“±ê¸‰ì€ " + df['Pclass'].astype(str) + "ë“±ê¸‰ì´ì—ˆìœ¼ë©°, ìµœì¢…ì ìœ¼ë¡œ " +
      df['Survived_str'] + "í–ˆìŠµë‹ˆë‹¤."
  )
  
  # LangChain ë¬¸ì„œ ê°ì²´ ë³€í™˜
  loader = DataFrameLoader(df, page_content_column='combined_info')
  docs = loader.load()
  
  # í…ìŠ¤íŠ¸ ë¶„í• 
  text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50, length_function=len)
  split_docs = text_splitter.split_documents(docs)
  
  print(f"ì›ë³¸ ë¬¸ì„œ ìˆ˜: {len(docs)}")
  print(f"ë¶„í• ëœ ë¬¸ì„œ ìˆ˜: {len(split_docs)}")
  
  ```

- ì„ë² ë”©

  ```
  model_name = "BAAI/bge-m3"
  device = 'cuda' if torch.cuda.is_available() else 'cpu'
  print(f"ì„ë² ë”© ì¥ì¹˜: {device}")
  
  embeddings = HuggingFaceBgeEmbeddings(
      model_name=model_name,
      model_kwargs={'device': device},
      encode_kwargs={'normalize_embeddings': True}
  )
  
  ```
  
  
  
- ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° minio ë²„í‚·ì— ì—…ë¡œë“œ

  ```yaml
  # MinIO í™˜ê²½ ë³€ìˆ˜
  os.environ['AWS_ACCESS_KEY_ID'] = 'minio'
  os.environ['AWS_SECRET_ACCESS_KEY'] = 'minio123'
  os.environ['S3_ENDPOINT_URL'] = 'https://minio-api-minio.apps.cluster-8882l.8882l.sandbox1647.opentlc.com'
  os.environ['S3_BUCKET'] = 'rag-with-langchain'
  
  # boto3 í´ë¼ì´ì–¸íŠ¸ ìƒì„±
  s3_client = boto3.client(
      's3',
      endpoint_url=os.environ['S3_ENDPOINT_URL'],
      aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],
      aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY']
  )
  
  # ì˜ˆì‹œ ëª¨ë¸ íŒŒì¼ ì—…ë¡œë“œ (ì„ íƒ)
  local_file_path = 'my_model.pt'
  s3_file_key = 'models/my_model.pt'
  try:
      s3_client.upload_file(local_file_path, os.environ['S3_BUCKET'], s3_file_key)
      print("âœ… ëª¨ë¸ íŒŒì¼ ì—…ë¡œë“œ ì„±ê³µ!")
  except FileNotFoundError:
      print(f"âŒ '{local_file_path}' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
  
  ```
  
- ë²¡í„°ìŠ¤í† ì–´ s3 ì—…ë¡œë“œ

  ```
  vectorstore_dir = "./chroma_persist"
  
  try:
      vectorstore = Chroma.from_documents(
          documents=split_docs,
          embedding=embeddings,
          collection_name="rag-with-langchain",
          persist_directory=vectorstore_dir
      )
      vectorstore.persist()
  
      # ì „ì²´ íŒŒì¼ S3 ì—…ë¡œë“œ
      for root, dirs, files in os.walk(vectorstore_dir):
          for file in files:
              local_path = os.path.join(root, file)
              s3_path = os.path.relpath(local_path, vectorstore_dir)
              s3_client.upload_file(local_path, os.environ['S3_BUCKET'], s3_path)
  
      print(f"âœ… MinIO ë²„í‚· '{os.environ['S3_BUCKET']}'ì— Chroma ë°ì´í„° ì €ì¥ ì™„ë£Œ!")
  except Exception as e:
      print(f"âŒ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
  
  ```
  
  
  
- RAG êµ¬ì„±

  ```yaml
  retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
  
  ollama_url = "http://ollama-server-service.ollama-dist.svc.cluster.local:11434"
  llm = ChatOllama(model="llama3", base_url=ollama_url)
  
  template = """
  ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ì¹œì ˆí•œ í•œêµ­ì–´ ì±—ë´‡ì…ë‹ˆë‹¤.
  ì£¼ì–´ì§„ ë¬¸ë§¥(context)ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸(question)ì— í•œêµ­ì–´ë¡œë§Œ ë‹µí•˜ì„¸ìš”.
  ë§Œì•½ ë¬¸ë§¥ì— ê´€ë ¨ ì •ë³´ê°€ ì—†ê±°ë‚˜ ì§ˆë¬¸ì— ë‹µí•  ìˆ˜ ì—†ë‹¤ë©´, ëª¨ë¥¸ë‹¤ê³  ë‹µë³€í•˜ì„¸ìš”.
  
  ë¬¸ë§¥: {context}
  
  ì§ˆë¬¸: {question}
  
  ë‹µë³€:
  """
  prompt = PromptTemplate.from_template(template)
  
  rag_chain = (
      {"context": retriever, "question": RunnablePassthrough()}
      | prompt
      | llm
      | StrOutputParser()
  )
  
  ```
  
- ì±—ë´‡ UI êµ¬ì„±

  ```
  # ------------------------------------------------------------
  # ì…€ 5: Gradio UI êµ¬ì„±
  # ------------------------------------------------------------
  def chat_fn(message, history):
      try:
          answer = rag_chain.invoke(message)
          return answer
      except Exception as e:
          return f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {e}"
  
  with gr.Blocks() as demo:
      gr.Markdown("# ğŸ’¬ RAG ê¸°ë°˜ ì›¹ ì±—ë´‡")
      chatbot = gr.Chatbot()
      msg = gr.Textbox(label="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")
      clear = gr.Button("ëŒ€í™” ì´ˆê¸°í™”")
  
      def respond(user_message, chat_history):
          bot_message = chat_fn(user_message, chat_history)
          chat_history.append((user_message, bot_message))
          return "", chat_history
  
      msg.submit(respond, [msg, chatbot], [msg, chatbot])
      clear.click(lambda: None, None, chatbot, queue=False)
  
  ```
  
  

