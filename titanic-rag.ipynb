{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 설치\n",
    "!pip install pandas langchain langchain-community langchain-chroma sentence-transformers torch\n",
    "\n",
    "import pandas as pd\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. 데이터 로드 및 전처리\n",
    "# -----------------------------------------------------------------------------\n",
    "# 사용자가 제공한 'titanic.csv' 파일을 불러옵니다.\n",
    "csv_file_path = './data/titanic.csv'\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# 챗봇이 더 풍부한 정보를 제공할 수 있도록 여러 컬럼을 결합하여 새로운 컬럼을 생성합니다.\n",
    "# 'Name', 'Sex', 'Age', 'Pclass', 'Survived' 정보를 결합하여 'combined_info' 컬럼을 만듭니다.\n",
    "# Survived 컬럼의 0과 1을 '사망'과 '생존'으로 변환하여 가독성을 높입니다.\n",
    "df['Survived_str'] = df['Survived'].apply(lambda x: '생존' if x == 1 else '사망')\n",
    "\n",
    "df['combined_info'] = (\n",
    "    df['Name'] + \"은(는) \" +\n",
    "    df['Sex'] + \"성 승객으로, 나이는 \" + df['Age'].astype(str) + \"세입니다. \" +\n",
    "    \"탑승 등급은 \" + df['Pclass'].astype(str) + \"등급이었으며, 최종적으로 \" +\n",
    "    df['Survived_str'] + \"했습니다.\"\n",
    ")\n",
    "\n",
    "# LangChain의 DataFrameLoader를 사용하여 DataFrame을 'Document' 객체로 변환합니다.\n",
    "# 'page_content_column'을 새로 만든 'combined_info' 컬럼으로 지정합니다.\n",
    "loader = DataFrameLoader(df, page_content_column='combined_info')\n",
    "docs = loader.load()\n",
    "\n",
    "# 문서가 너무 길 경우, 검색 정확도를 높이기 위해 작은 단위로 나눕니다.\n",
    "# RecursiveCharacterTextSplitter는 다양한 구분자를 활용하여 문서를 나눕니다.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,        # 한 덩어리의 최대 크기\n",
    "    chunk_overlap=50,      # 덩어리 간의 중복 크기\n",
    "    length_function=len\n",
    ")\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"원본 문서 수: {len(docs)}\")\n",
    "print(f\"분할된 문서 수: {len(split_docs)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. 임베딩 모델 및 벡터 스토어 설정\n",
    "# -----------------------------------------------------------------------------\n",
    "# BGE 한글 임베딩 모델을 불러옵니다.\n",
    "# 'bge-large-ko'는 한글에 최적화된 모델 중 하나입니다.\n",
    "# model_kwargs는 GPU를 사용할 경우 'cuda'로 설정할 수 있습니다.\n",
    "model_name = \"BAAI/bge-large-ko\"\n",
    "model_kwargs = {'device': 'cpu'}  # GPU 사용 시 'cuda'로 변경\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# ChromaDB를 사용하여 임베딩된 문서를 저장합니다.\n",
    "# 'persist_directory'를 지정하면 데이터가 디스크에 저장되어 다음에 재활용할 수 있습니다.\n",
    "persist_directory = 'chroma_db'\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=split_docs,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "# 쿼리에 가장 유사한 문서를 찾아주는 'retriever'를 생성합니다.\n",
    "# 'k=3'은 가장 유사한 3개의 문서를 가져오겠다는 의미입니다.\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 3. LLM 모델 및 RAG Chain 구성\n",
    "# -----------------------------------------------------------------------------\n",
    "# Ollama를 사용하여 로컬에서 실행 중인 LLM을 불러옵니다.\n",
    "# 'llama3' 대신 'beomi/KoAlpaca-Polyglot-5.8B'와 같은 한글 모델을 사용할 수 있습니다.\n",
    "# https://ollama.com/library/llama3\n",
    "# https://ollama.com/library/koalpaca\n",
    "# ollama pull llama3 또는 ollama pull koalpaca-polyglot:latest\n",
    "#\n",
    "# 만약 Ollama 설치가 어렵다면, 주석 처리하고 다른 LLM API를 사용하거나,\n",
    "# HuggingFaceHub를 사용해 API 호출을 할 수도 있습니다.\n",
    "# from langchain_community.llms import HuggingFaceHub\n",
    "# llm = HuggingFaceHub(repo_id=\"beomi/KoAlpaca-Polyglot-5.8B\", ...)\n",
    "\n",
    "llm = ChatOllama(model=\"llama3\")\n",
    "\n",
    "# RAG 시스템에 사용될 프롬프트 템플릿을 정의합니다.\n",
    "# 'context'와 'question' 변수를 사용하여 LLM에게 배경 정보와 질문을 함께 전달합니다.\n",
    "template = \"\"\"\n",
    "당신은 사용자의 질문에 답하는 친절한 한국어 챗봇입니다.\n",
    "주어진 문맥(context)을 사용하여 질문(question)에 답하세요.\n",
    "만약 문맥에 관련 정보가 없거나 질문에 답할 수 없다면, 모른다고 답변하세요.\n",
    "\n",
    "문맥: {context}\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "답변:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# RAG Chain을 구성합니다.\n",
    "# 1. RunnablePassthrough: 사용자의 질문을 retriever와 prompt로 전달합니다.\n",
    "# 2. retriever: 질문에 맞는 문서를 검색합니다.\n",
    "# 3. prompt: 검색된 문서를 context로, 질문을 question으로 넣어 프롬프트를 만듭니다.\n",
    "# 4. llm: 완성된 프롬프트를 기반으로 답변을 생성합니다.\n",
    "# 5. StrOutputParser: LLM의 응답을 문자열로 파싱합니다.\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"챗봇이 준비되었습니다. '종료'를 입력하면 대화가 끝납니다.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 4. 챗봇 대화 루프\n",
    "# -----------------------------------------------------------------------------\n",
    "while True:\n",
    "    try:\n",
    "        user_question = input(\"질문하세요: \")\n",
    "        if user_question.lower() == '종료':\n",
    "            print(\"대화를 종료합니다.\")\n",
    "            break\n",
    "\n",
    "        # RAG Chain을 실행하여 답변을 얻습니다.\n",
    "        result = rag_chain.invoke(user_question)\n",
    "        print(\"챗봇 답변:\")\n",
    "        pprint(result)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"오류가 발생했습니다: {e}\")\n",
    "        break\n",
    "\n",
    "# 사용이 끝난 벡터 스토어 데이터 삭제 (선택 사항)\n",
    "# import shutil\n",
    "# if os.path.exists(persist_directory):\n",
    "#     shutil.rmtree(persist_directory)\n",
    "#     print(f\"'{persist_directory}' 디렉토리를 삭제했습니다.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
